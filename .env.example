# Soprano-RS Environment Configuration
#
# Copy this file to .env and customize for your deployment
# All variables are optional - defaults are shown as comments

# =============================================================================
# Server Configuration
# =============================================================================

# Host address to bind the server to
# Default: 0.0.0.0
SOPRANO_HOST=0.0.0.0

# Port to listen on
# Default: 8080
SOPRANO_PORT=8080

# =============================================================================
# Model Configuration
# =============================================================================

# Path to local model directory
# Default: models
SOPRANO_MODEL_PATH=models

# HuggingFace model ID for automatic download
# Default: ekwek/Soprano-1.1-80M
SOPRANO_MODEL_ID=ekwek/Soprano-1.1-80M

# Device to use for inference: cpu, cuda, or metal
# Default: cuda (falls back to cpu if unavailable)
SOPRANO_DEVICE=cuda

# Enable automatic model download from HuggingFace
# Default: true
SOPRANO_DOWNLOAD=true

# Cache directory for downloaded models
# Default: ~/.cache/soprano-rs
SOPRANO_CACHE_DIR=~/.cache/soprano-rs

# =============================================================================
# Performance Configuration
# =============================================================================

# Number of decoder worker threads
# Higher values increase throughput but use more memory
# Default: 2
SOPRANO_WORKERS=2

# Maximum concurrent TTS requests in flight
# Default: same as workers
SOPRANO_TTS_INFLIGHT=2

# Decoder batch size (advanced)
# Default: 1
SOPRANO_DECODER_BATCH_SIZE=1

# =============================================================================
# Text Chunking Configuration
# =============================================================================

# Minimum words required before emitting a chunk
# Higher values reduce API calls but increase latency
# Default: 2
SOPRANO_MIN_WORDS=2

# Minimum characters before emitting a chunk
# Default: 24
SOPRANO_MIN_CHARS=24

# Maximum characters per chunk (hard limit)
# Default: 160
SOPRANO_MAX_CHARS=160

# Maximum delay in milliseconds before emitting a partial chunk
# Lower values reduce latency but may produce shorter chunks
# Default: 220
SOPRANO_MAX_DELAY_MS=220

# Maximum text length per request (characters)
# Default: 10000
SOPRANO_MAX_TEXT_LENGTH=10000

# =============================================================================
# Generation Configuration
# =============================================================================

# Temperature for sampling (0.0 = deterministic)
# Range: 0.0 - 2.0
# Default: 0.0
SOPRANO_TEMPERATURE=0.0

# Top-p (nucleus sampling) parameter
# Range: 0.0 - 1.0
# Default: 0.95
SOPRANO_TOP_P=0.95

# Repetition penalty (1.0 = no penalty)
# Range: 1.0 - 2.0
# Default: 1.2
SOPRANO_REPETITION_PENALTY=1.2

# Output sample rate in Hz
# Default: 32000
SOPRANO_SAMPLE_RATE=32000

# =============================================================================
# Logging Configuration
# =============================================================================

# Log level: error, warn, info, debug, trace
# Default: info
RUST_LOG=info

# Alternative log level setting (same as RUST_LOG)
# Default: info
SOPRANO_LOG=info

# =============================================================================
# Advanced Configuration
# =============================================================================

# Include source text in audio frame headers
# Useful for debugging but increases bandwidth
# Default: false
SOPRANO_INCLUDE_TEXT=false

# Inference timeout in milliseconds
# Default: 60000 (60 seconds)
SOPRANO_TIMEOUT_MS=60000

# HuggingFace endpoint URL (for private HF deployments)
# Default: https://huggingface.co
SOPRANO_HF_ENDPOINT=https://huggingface.co

# HuggingFace access token (for private models)
# Get yours from: https://huggingface.co/settings/tokens
# HF_TOKEN=your_token_here

# =============================================================================
# GPU Configuration (NVIDIA CUDA)
# =============================================================================

# Restrict visible CUDA devices
# Format: comma-separated list of GPU IDs
# Example: "0" or "0,1" or "1,2"
# CUDA_VISIBLE_DEVICES=0

# =============================================================================
# Docker/Container Configuration
# =============================================================================

# These are typically set by Docker, not manually
# HOSTNAME=your-hostname
# PATH=/usr/local/bin:/usr/bin:/bin

# =============================================================================
# Example Configurations for Different Scenarios
# =============================================================================

# --- High-Throughput Production ---
# SOPRANO_WORKERS=4
# SOPRANO_TTS_INFLIGHT=8
# SOPRANO_DEVICE=cuda
# RUST_LOG=warn

# --- Low-Latency Real-time ---
# SOPRANO_MIN_WORDS=1
# SOPRANO_MIN_CHARS=10
# SOPRANO_MAX_DELAY_MS=50
# SOPRANO_WORKERS=2

# --- CPU-Only Deployment ---
# SOPRANO_DEVICE=cpu
# SOPRANO_WORKERS=1
# SOPRANO_TTS_INFLIGHT=1

# --- Development/Debug ---
# RUST_LOG=debug
# SOPRANO_INCLUDE_TEXT=true
# SOPRANO_DOWNLOAD=true
